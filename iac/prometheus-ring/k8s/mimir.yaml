# Gets mimir S3 from secret
global: 
  extraEnvFrom:
    - secretRef:
        name: mimir-s3-credentials

# The "mimir" defines the mimir.yaml, that configures the Mimir components
# Note that this is the components configurations, not their 
# k8s setup  
mimir:
  # This section defines configuration for all components
  structuredConfig:
    common:
      storage:
        backend: s3
        s3:
          endpoint: ${S3_ENDPOINT}
          region: ${S3_REGION}
          access_key_id: ${S3_ACCESS_KEY_ID}
          secret_access_key: ${S3_SECRET_ACCESS_KEY}

    blocks_storage:
      s3:
        bucket_name: prom-ring-mimir-blocks
      tsdb:
        retention_period: 2h    # Setting a small period to force sending to s3 earlier
    alertmanager_storage:
      s3:
        bucket_name: prom-ring-mimir-alertmanager
    ruler_storage:
      s3:
        bucket_name: prom-ring-mimir-ruler
    distributor:
      # Configures consul as a kv store to mimir deduplication feature
      ha_tracker:
        enable_ha_tracker: true
        kvstore:
          store: consul
          consul:
            host: http://consul-server.consul.svc.cluster.local:8500
    

    # Mimir has relativelly low limits compared to the stress tests made here.
    # This ensures that it works on maximum capacity
    limits:
      # accept_ha_samples: true
      max_global_series_per_user: 0
      max_global_series_per_metric: 0
      max_fetched_chunks_per_query: 0
      accept_ha_samples: true
      # ingestion_rate: 100000000
      ingestion_rate: 10000000                    # Tenant ingestion limit  
      ingestion_burst_size: 2000000               # Default value
      ingestion_burst_factor: 10                  # Multiplier of the default ingestion_burst_size, i.e. 1000 * 200000 = 200000000
      # ingestion_burst_factor: 1000                # Multiplier of the default ingestion_burst_size, i.e. 1000 * 200000 = 200000000
      max_exemplars_per_series_per_request: 0     # Remove limits of queries
      
      # Defines label to be used in the deduplication feature
      # Don't know why this is configure under "limits", but it is what it is
      ha_cluster_label: cluster
      ha_replica_label: __replica__

# Enable metamonitoring (i.e. to monitor the monitor)
metaMonitoring:
  serviceMonitor:
    enabled: true
    labels:
      release: kube-prometheus-stack

# Ingester
ingester:
  # persistentVolume:
  #   size: 50Gi
  # replicas: 3
  # resources:
  #   limits:
  #     # cpu: 2.5
  #     memory: 30Gi
  #   requests:
  #     cpu: 2
  #     memory: 20Gi
  persistentVolume:
    size: 25Gi
  replicas: 18
  resources:
    limits:
      # cpu: 2.5
      memory: 20Gi
    requests:
      cpu: 1
      memory: 12Gi
  topologySpreadConstraints: {}
  affinity: &spreadReplicasAcrossNodes
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: target # support for enterprise.legacyLabels
                operator: In
                values:
                  - ingester
          topologyKey: 'kubernetes.io/hostname'

        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - ingester
          topologyKey: 'kubernetes.io/hostname'
  zoneAwareReplication:
    topologyKey: 'kubernetes.io/hostname'
  tolerations: &mimirToleration
    - key: "role"
      operator: "Equal"
      value: "mimir"
      effect: "NoSchedule"
    - key: "mimir-role"
      operator: "Equal"
      value: "ingester"
      effect: "NoSchedule"

# Store Gateway
store_gateway:
  persistentVolume:
    size: 10Gi
  replicas: 3
  resources:
    limits:
      # cpu: 1.4
      memory: 2.1Gi
    requests:
      cpu: 1
      memory: 1.5Gi
  topologySpreadConstraints: {}
  affinity: *spreadReplicasAcrossNodes
  zoneAwareReplication:
    topologyKey: 'kubernetes.io/hostname'
  tolerations:  &mimirToleration
    - key: "role"
      operator: "Equal"
      value: "mimir"
      effect: "NoSchedule"

# Compactor
compactor:
  persistentVolume:
    size: 20Gi
  resources:
    limits:
      cpu: 4
      memory: 2.1Gi
    requests:
      cpu: 1
      memory: 1.0Gi
  tolerations: *mimirToleration

# Distributor
distributor:
  replicas: 2
  resources:
    limits:
      # cpu: 2
      memory: 5.7Gi
    requests:
      cpu: 2
      memory: 4Gi
  tolerations: *mimirToleration

querier:
  replicas: 1  # subject to change
  resources:
    limits:
      # cpu: 2.8
      memory: 5.6Gi
    requests:
      cpu: 1
      memory: 2Gi
  tolerations: *mimirToleration

query_frontend:
  replicas: 1
  resources:
    limits:
      # cpu: 2.8
      memory: 2.8Gi
    requests:
      cpu: 0.5
      memory: 1Gi
  tolerations: *mimirToleration

nginx:
  replicas: 1
  resources:
    limits:
      # cpu: 1.4
      memory: 1Gi
    requests:
      cpu: 0.5
      memory: 500Mi
  tolerations: *mimirToleration

# Gateway
gateway:
  enabledNonEnterprise: true
  enabled: true
  replicas: 1
  resources:
    limits:
      # cpu: 1
      memory: 512Mi
    requests:
      cpu: 1
      memory: 256Mi
  service:
    type: LoadBalancer
    port: 80
  tolerations: *mimirToleration

chunks-cache:
  enabled: true
  replicas: 2
  tolerations: *mimirToleration

index-cache:
  enabled: true
  replicas: 3
  tolerations: *mimirToleration

metadata-cache:
  enabled: true
  tolerations: *mimirToleration

results-cache:
  enabled: true
  tolerations: *mimirToleration

# Disables Minio to use the provider s3
minio:
  enabled: false

alertmanager:
  enabled: false

ruler:
  enabled: false

overrides_exporter:
  enabled: false

query_scheduler:
  enabled: false

admin_api:
  enable: false

admin-cache:
  enabled: false

rollout_operator:
  enabled: false